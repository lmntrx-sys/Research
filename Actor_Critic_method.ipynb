{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFEgcbKS8Tqux3nbIh4uuo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmntrx-sys/Research/blob/main/Actor_Critic_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "aVbF1cRN81tX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEA_WMgr8VrE",
        "outputId": "11198944-30e5-4d1c-c7aa-e3c783e6464f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7VFwPsUK801G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "# Environment\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "env.reset(seed=42)\n",
        "\n",
        "# Train params\n",
        "max_iters_per_episode = 1000 # Adjust as needed\n",
        "n_max_episodes = 1000 # Adjust as needed\n",
        "discount_factor = 0.99\n",
        "\n",
        "# Smallest number such that 1.0 + eps != 1.0\n",
        "eps = np.finfo(np.float32).eps"
      ],
      "metadata": {
        "id": "kJ0KS0cL9Hd0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render():\n",
        "  state_image = env.render()\n",
        "  plt.imshow(state_image)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Ch8ARZsfMcHl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent\n",
        "\n",
        "# Get the shape of the environment\n",
        "obs_shape = env.observation_space.shape\n",
        "print(\"Observation shape:\", obs_shape)\n",
        "\n",
        "# Get the number of actions\n",
        "n_actions = env.action_space.n\n",
        "print(\"Number of actions:\", n_actions)\n",
        "\n",
        "obs_shape = 4\n",
        "n_actions = 2\n",
        "\n",
        "# Small neural network\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(obs_shape,)),\n",
        "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(n_actions, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WVqZlMb-HdQ",
        "outputId": "97bf32b8-cd12-44a0-c63e-3a4cbac11992"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation shape: (4,)\n",
            "Number of actions: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "B1C2arY-AY7p",
        "outputId": "1598b9fb-486f-4b27-9318-cd32554aea49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m40\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m18\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58\u001b[0m (232.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58</span> (232.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58\u001b[0m (232.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58</span> (232.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "critic = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(obs_shape,)),\n",
        "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "ainOcGUB9KOF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network Params\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.Huber()\n",
        "reward_history = []\n",
        "running_rewards = 0.0\n",
        "running_episodes = 0\n",
        "action_probs_history = []\n",
        "critic_value_history = []\n",
        "action_history = []"
      ],
      "metadata": {
        "id": "43wQn46nA7JJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "source": [
        "# Train our model\n",
        "\n",
        "for episode in range(n_max_episodes):\n",
        "    obs, info = env.reset()\n",
        "    episode_reward = 0\n",
        "    for step in range(max_iters_per_episode):\n",
        "      with tf.GradientTape() as tape:\n",
        "\n",
        "        state = tf.convert_to_tensor(obs)\n",
        "        state = tf.expand_dims(state, 0)\n",
        "\n",
        "        # Predict action probabilities\n",
        "        action_probabilities = model(state)\n",
        "        critic_value = critic(state)\n",
        "        critic_value_history.append(critic_value[0, 0])\n",
        "\n",
        "        # Sample next action from action probabilities\n",
        "        action = np.random.choice(n_actions, p=np.squeeze(action_probabilities))\n",
        "        action_probs_history.append(tf.keras.ops.log(action_probabilities[0, action]))\n",
        "\n",
        "        # Apply the action\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "        reward_history.append(reward)\n",
        "\n",
        "        if terminated or truncated:\n",
        "          break\n",
        "\n",
        "\n",
        "        # Update running reward to check condition for solving\n",
        "        running_rewards = 0.05 * episode_reward + (1 - 0.05) * running_rewards\n",
        "\n",
        "        # Calculate discounted future rewards\n",
        "        discounts = np.array([discount_factor ** i for i in range(len(reward_history))])\n",
        "        future_rewards = np.array(reward_history) * discounts # Use actual reward history\n",
        "        returns = (future_rewards - np.mean(future_rewards)) / (np.std(future_rewards) + eps)\n",
        "\n",
        "        # Use a dummy critic value for now\n",
        "        critic_value_history = [0.0] * len(reward_history)\n",
        "\n",
        "        history = zip(action_probs_history, critic_value_history, returns)\n",
        "\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "\n",
        "        for log_prob, value, ret in history:\n",
        "            # At this point in history, the critic estimated that we would get a\n",
        "            # total reward = `value` in the future. We took an action with log probability\n",
        "            # of `log_prob` and ended up receiving a total reward = `ret`.\n",
        "            # The actor must be updated so that it predicts an action that leads to\n",
        "            # high rewards (compared to critic's estimate) with high probability.\n",
        "            diff = ret - value\n",
        "            actor_losses.append(-log_prob * diff)  # actor loss\n",
        "\n",
        "            # The critic must be updated so that it predicts a better estimate of\n",
        "            # the future rewards.\n",
        "            critic_losses.append(\n",
        "                loss_fn(tf.keras.ops.expand_dims(value, 0), tf.keras.ops.expand_dims(ret, 0))\n",
        "            )\n",
        "\n",
        "        # Backpropagation\n",
        "        loss_value = sum(actor_losses)  + sum(critic_losses)\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "\n",
        "\n",
        "        # Clear the loss and reward history\n",
        "        action_probs_history.clear()\n",
        "        critic_value_history.clear()\n",
        "        reward_history.clear()\n",
        "\n",
        "    # Log details\n",
        "    running_episodes += 1\n",
        "    if running_episodes % 10 == 0:\n",
        "        template = \"running reward: {:.2f} at episode {}\"\n",
        "        print(template.format(running_rewards, running_episodes))\n",
        "\n",
        "    if running_rewards > 195:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(running_episodes))\n",
        "        break"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7wtz7bKN6n6",
        "outputId": "01857f24-796a-4060-ad29-39ebdf958aaa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 12.98 at episode 10\n",
            "running reward: 7.77 at episode 20\n",
            "running reward: 9.79 at episode 30\n",
            "running reward: 15.30 at episode 40\n",
            "running reward: 10.82 at episode 50\n",
            "running reward: 8.97 at episode 60\n",
            "running reward: 19.22 at episode 70\n",
            "running reward: 8.56 at episode 80\n",
            "running reward: 13.11 at episode 90\n",
            "running reward: 15.12 at episode 100\n",
            "running reward: 11.97 at episode 110\n",
            "running reward: 10.65 at episode 120\n",
            "running reward: 10.24 at episode 130\n",
            "running reward: 11.34 at episode 140\n",
            "running reward: 15.43 at episode 150\n",
            "running reward: 9.37 at episode 160\n",
            "running reward: 13.30 at episode 170\n",
            "running reward: 13.88 at episode 180\n",
            "running reward: 12.47 at episode 190\n",
            "running reward: 7.82 at episode 200\n",
            "running reward: 9.89 at episode 210\n",
            "running reward: 14.96 at episode 220\n",
            "running reward: 10.83 at episode 230\n",
            "running reward: 9.50 at episode 240\n",
            "running reward: 9.24 at episode 250\n",
            "running reward: 14.63 at episode 260\n",
            "running reward: 6.73 at episode 270\n",
            "running reward: 10.32 at episode 280\n",
            "running reward: 28.94 at episode 290\n",
            "running reward: 9.82 at episode 300\n",
            "running reward: 7.99 at episode 310\n",
            "running reward: 14.26 at episode 320\n",
            "running reward: 6.59 at episode 330\n",
            "running reward: 9.61 at episode 340\n",
            "running reward: 8.21 at episode 350\n",
            "running reward: 11.93 at episode 360\n",
            "running reward: 11.88 at episode 370\n",
            "running reward: 10.10 at episode 380\n",
            "running reward: 10.83 at episode 390\n",
            "running reward: 7.79 at episode 400\n",
            "running reward: 8.30 at episode 410\n",
            "running reward: 9.35 at episode 420\n",
            "running reward: 25.37 at episode 430\n",
            "running reward: 10.01 at episode 440\n",
            "running reward: 9.96 at episode 450\n",
            "running reward: 10.91 at episode 460\n",
            "running reward: 16.62 at episode 470\n",
            "running reward: 10.22 at episode 480\n",
            "running reward: 9.51 at episode 490\n",
            "running reward: 9.44 at episode 500\n",
            "running reward: 16.31 at episode 510\n",
            "running reward: 9.47 at episode 520\n",
            "running reward: 13.21 at episode 530\n",
            "running reward: 11.33 at episode 540\n",
            "running reward: 9.41 at episode 550\n",
            "running reward: 10.15 at episode 560\n",
            "running reward: 7.09 at episode 570\n",
            "running reward: 7.90 at episode 580\n",
            "running reward: 10.09 at episode 590\n",
            "running reward: 7.98 at episode 600\n",
            "running reward: 9.06 at episode 610\n",
            "running reward: 8.93 at episode 620\n",
            "running reward: 9.69 at episode 630\n",
            "running reward: 13.30 at episode 640\n",
            "running reward: 13.70 at episode 650\n",
            "running reward: 15.77 at episode 660\n",
            "running reward: 9.86 at episode 670\n",
            "running reward: 13.38 at episode 680\n",
            "running reward: 7.77 at episode 690\n",
            "running reward: 9.65 at episode 700\n",
            "running reward: 12.32 at episode 710\n",
            "running reward: 12.09 at episode 720\n",
            "running reward: 22.77 at episode 730\n",
            "running reward: 11.28 at episode 740\n",
            "running reward: 11.36 at episode 750\n",
            "running reward: 8.93 at episode 760\n",
            "running reward: 11.46 at episode 770\n",
            "running reward: 9.64 at episode 780\n",
            "running reward: 16.17 at episode 790\n",
            "running reward: 11.40 at episode 800\n",
            "running reward: 10.17 at episode 810\n",
            "running reward: 8.31 at episode 820\n",
            "running reward: 9.61 at episode 830\n",
            "running reward: 7.93 at episode 840\n",
            "running reward: 8.74 at episode 850\n",
            "running reward: 10.60 at episode 860\n",
            "running reward: 8.68 at episode 870\n",
            "running reward: 6.84 at episode 880\n",
            "running reward: 6.66 at episode 890\n",
            "running reward: 8.52 at episode 900\n",
            "running reward: 12.84 at episode 910\n",
            "running reward: 8.77 at episode 920\n",
            "running reward: 10.93 at episode 930\n",
            "running reward: 7.33 at episode 940\n",
            "running reward: 9.28 at episode 950\n",
            "running reward: 15.78 at episode 960\n",
            "running reward: 12.44 at episode 970\n",
            "running reward: 9.04 at episode 980\n",
            "running reward: 8.95 at episode 990\n",
            "running reward: 8.72 at episode 1000\n"
          ]
        }
      ]
    }
  ]
}