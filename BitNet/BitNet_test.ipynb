{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BitLinearOptimized Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import SimpleLinear\n",
    "\n",
    "# Example usage\n",
    "input_shape = 64\n",
    "out_shape = 128\n",
    "sample_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(sample_size, input_shape)\n",
    "true_weights = torch.randn(input_shape, out_shape)\n",
    "\n",
    "# Generating synthetic targets: y = inputs x true_weights + noise\n",
    "noise = 0.05 * torch.randn(sample_size, out_shape)\n",
    "targets = inputs @ true_weights + noise\n",
    "\n",
    "inputs = inputs.to(torch.bfloat16)\n",
    "targets = targets.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16 torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(inputs.dtype, targets.dtype)\n",
    "\n",
    "model = SimpleLinear(input_shape, out_shape, num_groups=5)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 500\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/500], Loss: 1036.4595\n",
      "Epoch [100/500], Loss: 953.9650\n",
      "Epoch [200/500], Loss: 892.1829\n",
      "Epoch [300/500], Loss: 840.7665\n",
      "Epoch [400/500], Loss: 799.8828\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Convert outputs and targets to float32 for the loss computation\n",
    "    loss = criterion(outputs.to(torch.float32), targets.to(torch.float32))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dequantized Weights After Training: tensor([[-1., -1., -1.,  ...,  1., -1., -1.],\n",
      "        [ 1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [ 1., -1., -1.,  ...,  1., -1.,  1.],\n",
      "        [ 1.,  1., -1.,  ..., -1.,  1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "# Get the dequantized weights after training\n",
    "dequantized_weights_after_training = model.linear.dequantize_weights()\n",
    "\n",
    "# Print or use the dequantized weights as needed\n",
    "print(\"Dequantized Weights After Training:\", dequantized_weights_after_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Weights: tensor([[-0.0483, -0.0992, -0.0421,  ...,  0.0238, -0.1114, -0.0296],\n",
      "        [ 0.0373, -0.0164, -0.0545,  ..., -0.0303,  0.0957,  0.0761],\n",
      "        [-0.0624,  0.1110,  0.0243,  ...,  0.0245, -0.0934, -0.0966],\n",
      "        ...,\n",
      "        [-0.0462,  0.0201,  0.0166,  ...,  0.1026, -0.0393,  0.0914],\n",
      "        [ 0.1205, -0.0904, -0.0862,  ...,  0.0107, -0.1045,  0.0873],\n",
      "        [ 0.1176,  0.0041, -0.1205,  ..., -0.1180,  0.1044, -0.0639]])\n",
      "binarized weight:  tensor([[-1., -1., -1.,  ...,  1., -1., -1.],\n",
      "        [ 1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [ 1., -1., -1.,  ...,  1., -1.,  1.],\n",
      "        [ 1.,  1., -1.,  ..., -1.,  1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Retrieve original weights\n",
    "original_weights = model.linear.get_original_weights()\n",
    "print(\"Original Weights:\", original_weights)\n",
    "print(\"binarized weight: \", model.linear.dequantize_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BitLinear Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This linear implementation is different from the optimized implementation when it comes to data types during infrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[570.3411865234375, 572.9613037109375, 577.2484130859375, 566.1704711914062, 570.7861328125]\n",
      "Parameter containing:\n",
      "tensor([[ 1.9651e-03,  3.9742e-03,  6.0852e-03,  4.6182e-03,  4.5792e-03,\n",
      "          5.1233e-03,  3.8873e-03,  5.7519e-03,  7.9827e-03,  1.0004e-02,\n",
      "          5.2477e-03,  3.9558e-01,  1.6974e-03,  3.8776e-03,  6.2353e-03,\n",
      "          6.3895e-03,  1.0197e-03,  6.9853e-03,  2.6393e-03,  2.0338e-03],\n",
      "        [ 1.8776e-03,  1.6569e-03,  1.3384e+00,  1.9661e-01,  7.3371e-04,\n",
      "          7.1123e-03,  4.8208e-04,  1.0617e-03,  2.5296e-03,  1.3610e-03,\n",
      "          2.9625e-03,  4.6927e-03,  2.0718e-03,  5.7901e-03,  6.0808e-03,\n",
      "          4.6200e-03,  7.8272e-04,  8.4426e-03,  1.2481e-03,  2.7303e-03],\n",
      "        [ 5.6645e-03,  1.9017e-03,  1.4181e-03,  5.7114e-03,  5.3053e-03,\n",
      "          1.8370e-02,  6.2806e-03,  1.9101e-03,  6.7641e-03,  2.1519e-03,\n",
      "          4.0739e-03,  5.9303e-03,  1.5918e-03,  5.1723e-04,  2.7801e-03,\n",
      "          7.5162e-03,  4.1349e-03,  5.1004e-03,  2.5890e-01,  8.4162e-03],\n",
      "        [ 1.6971e-03,  2.3493e-03,  2.0459e-03,  3.8971e-03,  3.1064e-03,\n",
      "          5.8898e-03,  1.5290e-03,  2.2889e-03,  6.0993e-03,  3.0436e-02,\n",
      "          6.5585e-03,  8.6312e-03,  2.4256e-03,  2.3784e-03,  3.5215e-03,\n",
      "          2.3381e-03,  1.7930e-03,  2.2413e-03,  2.0031e-03,  3.4753e-03],\n",
      "        [ 5.5957e-03,  3.1359e-03,  5.4031e-03,  3.1567e-03,  1.9745e-03,\n",
      "          9.1150e-03,  7.4898e-03,  2.6693e-03,  7.0408e-04,  2.5139e-03,\n",
      "          4.6021e-03,  4.3094e-03,  4.4369e-03,  1.7816e-03,  2.2526e-03,\n",
      "          1.5040e-03,  3.0262e-03,  4.3654e-04,  1.3251e-03,  3.4709e-01],\n",
      "        [-1.8768e+00, -1.7804e+00, -1.8136e+00,  1.9151e+00,  1.9633e+00,\n",
      "          1.8963e+00,  1.7632e+00, -1.9158e+00, -1.9380e+00,  1.9083e+00,\n",
      "          2.0178e+00, -1.8677e+00,  1.9638e+00, -1.9225e+00,  1.8967e+00,\n",
      "         -1.8585e+00,  6.5648e-01, -1.8738e+00,  2.0653e-03, -1.8641e+00],\n",
      "        [ 8.0176e-03,  5.5927e-03,  8.5977e-03,  6.6235e-03,  5.3861e-03,\n",
      "          1.0655e-02,  9.2038e-03,  8.5499e-03,  7.8339e-03,  6.3995e-03,\n",
      "          1.1792e-02,  7.0243e-03,  8.1053e-03,  6.0665e-03,  8.0379e-03,\n",
      "          8.6372e-03,  6.1854e-03,  7.4355e-03,  7.4537e-03,  6.1898e-03],\n",
      "        [ 6.5214e-03,  7.9144e-03,  7.3335e-03,  6.9712e-03,  8.5664e-03,\n",
      "          9.1650e-03,  6.7019e-03,  9.5096e-03,  7.3823e-03,  7.9509e-03,\n",
      "          7.4022e-03,  8.5230e-03,  9.3041e-03,  6.2243e-03,  6.9134e-03,\n",
      "          7.2276e-03,  6.8113e-03,  9.0947e-03,  8.6493e-03,  6.6023e-03],\n",
      "        [ 5.2956e-03,  6.9016e-03,  1.0555e-02,  5.3671e-03,  9.4884e-03,\n",
      "          6.1147e-03,  1.0748e-02,  7.5011e-03,  5.5744e-03,  9.4450e-03,\n",
      "          6.0603e-03,  1.0791e-02,  6.3290e-03,  8.7049e-03,  6.7855e-03,\n",
      "          7.2122e-03,  1.1547e-02,  7.6282e-03,  8.8295e-03,  7.2034e-03],\n",
      "        [ 1.1329e-02,  8.4595e-03,  8.4782e-03,  7.2077e-03,  7.9163e-03,\n",
      "          9.9695e-03,  9.5329e-03,  6.6001e-03,  6.7152e-03,  5.7832e-03,\n",
      "          9.1398e-03,  9.9127e-03,  8.6272e-03,  7.8978e-03,  6.6820e-03,\n",
      "          9.0259e-03,  1.1079e-02,  8.0981e-03,  6.7971e-03,  8.7227e-03],\n",
      "        [ 7.7584e-03,  9.6877e-03,  6.6301e-03,  9.1469e-03,  7.2445e-03,\n",
      "          5.8154e-03,  7.5321e-03,  7.5618e-03,  7.6980e-03,  5.4674e-03,\n",
      "          6.8660e-03,  7.9904e-03,  8.0872e-03,  7.2627e-03,  7.3323e-03,\n",
      "          5.9347e-03,  1.0594e-02,  9.1016e-03,  8.6909e-03,  9.1745e-03],\n",
      "        [ 7.8688e-03,  8.4022e-03,  6.6876e-03,  1.0256e-02,  8.2500e-03,\n",
      "          1.0198e-02,  8.0685e-03,  8.2547e-03,  7.6433e-03,  8.1299e-03,\n",
      "          6.4079e-03,  7.3181e-03,  8.8066e-03,  9.0561e-03,  9.0868e-03,\n",
      "          7.7824e-03,  8.2988e-03,  9.5865e-03,  7.8657e-03,  7.1302e-03],\n",
      "        [-1.6148e-01, -1.6502e-01, -7.3938e-01, -1.6468e-01, -1.6518e-01,\n",
      "         -1.6452e-01, -1.6234e-01, -1.6741e-01, -1.6579e-01, -3.9267e-01,\n",
      "         -1.7069e-01, -1.6487e-01, -1.6884e-01, -1.6513e-01, -1.6211e-01,\n",
      "         -8.3734e-01, -2.0622e-01, -1.8246e-01, -1.6334e-01, -1.6428e-01],\n",
      "        [ 1.6390e+00,  1.4499e+00, -2.0553e+00,  1.6310e+00,  5.1331e-01,\n",
      "         -1.5192e+00,  1.7744e+00, -1.2842e+00, -2.1291e+00, -1.7769e+00,\n",
      "          1.7511e+00,  1.6663e+00, -1.9057e+00,  1.8724e+00, -1.7336e+00,\n",
      "          1.4096e+00, -1.7942e+00, -1.7452e+00,  1.6932e+00,  1.7869e+00],\n",
      "        [-1.6448e-01, -1.6526e-01, -7.1817e-01, -1.6945e-01, -1.6167e-01,\n",
      "         -1.6323e-01, -1.6583e-01, -1.6647e-01, -1.6374e-01, -1.6837e-01,\n",
      "         -1.7263e-01, -1.6259e-01, -1.8256e-01, -1.6309e-01, -1.6161e-01,\n",
      "         -1.6338e-01, -1.6611e-01, -4.7338e-01, -1.6415e-01, -1.8134e-01],\n",
      "        [-1.6996e-01, -1.6993e-01, -1.7222e-01, -2.5317e-01, -1.6229e-01,\n",
      "         -1.6822e-01, -1.6797e-01, -1.6437e-01, -1.6726e-01, -4.5257e-01,\n",
      "         -1.6366e-01, -1.6260e-01, -1.6201e-01, -1.6378e-01, -1.6825e-01,\n",
      "         -1.6560e-01, -1.6651e-01, -7.9050e-01, -1.6709e-01, -1.7095e-01],\n",
      "        [-1.6342e-01, -1.6654e-01, -1.6751e-01, -1.9446e-01, -1.6472e-01,\n",
      "         -7.0234e-01, -1.7147e-01, -1.6396e-01, -1.6746e-01, -1.6220e-01,\n",
      "         -1.6431e-01, -1.6362e-01, -1.6409e-01, -1.6362e-01, -1.6434e-01,\n",
      "         -1.6498e-01, -1.6881e-01, -1.6478e-01, -1.6724e-01, -4.2154e-01],\n",
      "        [-1.6352e-01, -1.6951e-01, -1.6633e-01, -1.6699e-01, -1.6314e-01,\n",
      "         -1.6419e-01, -1.7317e-01, -1.7042e-01, -1.7553e-01, -1.6239e-01,\n",
      "         -1.6483e-01, -1.6259e-01, -1.6485e-01, -1.6356e-01, -1.6570e-01,\n",
      "         -1.6293e-01, -1.6417e-01, -1.6849e-01, -1.7300e-01, -1.6431e-01],\n",
      "        [-2.6029e-01, -2.6078e-01, -2.6399e-01, -2.7333e-01, -2.5923e-01,\n",
      "         -1.0742e+00, -2.6007e-01, -2.6090e-01, -2.6230e-01, -2.6769e-01,\n",
      "         -4.4104e-01, -1.0011e+00, -2.6154e-01, -2.5966e-01, -2.6230e-01,\n",
      "         -1.1749e+00, -2.6183e-01, -7.1688e-01, -2.6430e-01, -2.6094e-01],\n",
      "        [ 1.5062e+00,  1.5462e+00, -1.9299e+00, -1.9347e+00,  1.7496e+00,\n",
      "          1.3493e+00,  1.6993e+00, -9.6430e-01, -1.9107e+00,  1.6424e+00,\n",
      "          1.8434e+00, -1.9035e+00, -2.5862e-01,  3.9582e-01,  1.7175e+00,\n",
      "         -1.8946e+00,  1.4503e+00,  1.3092e+00, -2.6176e-01, -1.7925e+00],\n",
      "        [-2.6020e-01, -3.4541e-01, -1.1599e+00, -1.4744e+00, -2.6118e-01,\n",
      "         -2.6242e-01, -2.5903e-01, -2.5776e-01, -2.6592e-01, -2.6141e-01,\n",
      "         -2.6019e-01, -7.8939e-01, -2.6394e-01, -2.6171e-01, -2.6370e-01,\n",
      "         -2.6174e-01, -2.6764e-01, -1.2397e+00, -2.6649e-01, -2.6384e-01],\n",
      "        [ 1.7902e+00, -1.7877e+00,  1.8283e+00, -1.7881e+00, -1.7607e+00,\n",
      "         -1.7509e+00, -1.7958e+00,  1.1959e+00, -1.7877e+00,  3.5580e-02,\n",
      "          1.3363e+00,  1.7549e+00,  1.6856e+00,  1.8368e+00, -1.3547e+00,\n",
      "         -1.7016e+00, -2.5895e-01, -1.1011e+00, -1.7708e+00,  1.1834e+00],\n",
      "        [-2.6640e-01, -2.6038e-01, -2.6342e-01, -3.4427e-01, -2.6437e-01,\n",
      "         -2.6038e-01, -2.9907e-01, -2.6031e-01, -2.5967e-01, -2.6189e-01,\n",
      "         -3.2906e-01, -2.6002e-01, -2.6221e-01, -2.6048e-01, -2.5983e-01,\n",
      "         -2.6071e-01, -2.6473e-01, -3.1263e-01, -2.6367e-01, -4.3256e-01],\n",
      "        [-2.6339e-01, -2.6370e-01, -3.4986e-01, -7.2782e-01, -2.6365e-01,\n",
      "         -2.6687e-01, -2.5892e-01, -2.6003e-01, -2.7087e-01, -3.4389e-01,\n",
      "         -2.5897e-01, -2.5940e-01, -2.6300e-01, -2.6312e-01, -2.6945e-01,\n",
      "         -2.6501e-01, -9.8642e-01, -1.0353e+00, -2.6347e-01, -2.6312e-01],\n",
      "        [ 7.0180e-02, -5.4702e-02, -1.2100e-01,  5.6191e-02,  6.8127e-02,\n",
      "          7.2719e-02,  7.1809e-02,  7.4577e-02,  7.3685e-02,  6.3963e-02,\n",
      "          7.1617e-02,  7.0968e-02,  7.3500e-02,  7.2519e-02,  7.2960e-02,\n",
      "         -2.5758e-01,  7.1466e-02,  7.1414e-02,  6.3361e-02,  6.1966e-02],\n",
      "        [-1.7977e+00,  1.3188e+00,  1.6093e+00,  1.6010e+00,  7.5317e-02,\n",
      "         -1.7020e+00,  1.7831e+00, -1.4014e+00,  1.6328e+00, -1.8048e+00,\n",
      "          1.3810e+00,  7.4983e-02,  1.7007e+00,  7.8485e-02, -1.8068e+00,\n",
      "         -1.7025e+00, -1.6927e+00,  1.4033e+00,  1.6338e+00,  1.5664e+00],\n",
      "        [ 7.2478e-02,  7.1542e-02, -1.6210e-01,  6.9361e-02,  7.4682e-02,\n",
      "         -4.4970e-02,  6.8658e-02,  7.0686e-02,  6.7467e-02,  5.3340e-02,\n",
      "          7.1001e-02,  6.7178e-02,  7.1864e-02,  6.5968e-02,  7.2356e-02,\n",
      "          7.3187e-02,  7.3291e-02,  6.7993e-02,  4.8422e-02,  7.2101e-02],\n",
      "        [ 7.4276e-02,  7.2554e-02,  7.0662e-02, -7.2740e-02,  6.9807e-02,\n",
      "          6.7363e-02,  7.2821e-02,  6.9590e-02,  7.2461e-02,  6.7157e-02,\n",
      "          5.7921e-02, -2.3235e-03,  7.1433e-02,  7.2954e-02,  7.0725e-02,\n",
      "          5.1068e-02,  6.9086e-02,  7.2079e-02,  7.2828e-02,  6.4935e-02],\n",
      "        [ 7.0021e-02,  7.0926e-02, -1.0498e-01,  6.7146e-02,  7.2291e-02,\n",
      "         -8.3159e-02,  7.2831e-02,  6.5721e-02,  7.4173e-02,  7.2892e-02,\n",
      "          6.8806e-02,  7.0174e-02,  7.1463e-02,  6.5892e-02,  7.2174e-02,\n",
      "          6.7806e-02,  7.0661e-02, -1.8043e-01,  7.0790e-02, -1.4327e-01],\n",
      "        [ 7.3496e-02,  7.2480e-02,  7.2429e-02,  7.1596e-02,  7.3303e-02,\n",
      "          7.3390e-02,  7.2782e-02,  7.0942e-02,  7.2197e-02,  7.1842e-02,\n",
      "          7.3208e-02,  7.3557e-02,  7.2564e-02,  6.9521e-02,  6.8498e-02,\n",
      "         -8.1480e-02,  6.9969e-02,  6.9976e-02,  7.1982e-02,  7.0509e-02]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
      "          1.,  1., -1.,  1.,  1.,  1.],\n",
      "        [ 1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
      "          1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.,\n",
      "          1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "          1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
      "          1., -1.,  1., -1., -1.,  1.],\n",
      "        [-1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,\n",
      "          1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,\n",
      "          1.,  1., -1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,\n",
      "         -1., -1., -1.,  1.,  1., -1.],\n",
      "        [-1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
      "         -1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1.,  1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,\n",
      "         -1.,  1.,  1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1.,\n",
      "         -1., -1.,  1.,  1.,  1.,  1.],\n",
      "        [-1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,\n",
      "          1., -1.,  1.,  1., -1., -1.],\n",
      "        [ 1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "          1., -1., -1., -1., -1., -1.],\n",
      "        [ 1.,  1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,\n",
      "         -1.,  1., -1., -1.,  1.,  1.],\n",
      "        [-1., -1., -1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1.,\n",
      "          1., -1., -1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1.,\n",
      "         -1., -1., -1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,\n",
      "         -1., -1., -1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1.,\n",
      "         -1.,  1., -1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,\n",
      "         -1., -1., -1., -1., -1., -1.],\n",
      "        [ 1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,\n",
      "          1., -1.,  1.,  1., -1., -1.],\n",
      "        [-1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,\n",
      "         -1., -1., -1., -1., -1., -1.],\n",
      "        [ 1., -1.,  1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         -1., -1.,  1., -1., -1.,  1.],\n",
      "        [-1., -1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
      "          1., -1., -1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,\n",
      "         -1., -1., -1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,\n",
      "         -1., -1., -1., -1., -1., -1.],\n",
      "        [-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
      "         -1., -1., -1.,  1.,  1.,  1.],\n",
      "        [-1., -1., -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "         -1., -1.,  1., -1., -1., -1.],\n",
      "        [ 1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "         -1., -1., -1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
      "         -1., -1., -1., -1., -1., -1.],\n",
      "        [ 1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1.,\n",
      "         -1., -1., -1., -1., -1., -1.]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "from module_linear import BitLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 20]), torch.Size([1000, 30]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "input_size = 20\n",
    "output_size = 30\n",
    "num_samples = 1000\n",
    "\n",
    "# Random input data\n",
    "inputs = torch.randn(num_samples, input_size)\n",
    "\n",
    "# True weights for generating synthetic targets (randomly initialized)\n",
    "true_weights = torch.randn(input_size, output_size)\n",
    "\n",
    "# Generating synthetic targets: y = inputs x true_weights + noise\n",
    "noise = 0.05 * torch.randn(num_samples, output_size)\n",
    "targets = inputs @ true_weights + noise\n",
    "\n",
    "inputs.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a simple neural network with the BitLinear layer\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_groups):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.bitlinear = BitLinear(input_size, output_size, num_groups=num_groups)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.bitlinear(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNet(input_size, output_size, num_groups=5)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 200\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[560.5733032226562, 571.445556640625, 578.6713256835938, 587.3937377929688, 578.71337890625]\n",
      "Parameter containing:\n",
      "tensor([[ 1.1603e-02,  1.1009e-02,  1.1926e-02,  1.2520e-02,  1.2838e-02,\n",
      "          1.4962e-02,  1.2898e-02,  1.2050e-02,  1.5594e-02,  8.3676e-03,\n",
      "          1.5321e-02,  1.2816e-02,  1.2309e-02,  1.0993e-02,  1.4098e-02,\n",
      "          1.0930e-02,  1.2763e-02,  7.7721e-03,  9.2826e-03,  1.3228e-02],\n",
      "        [ 1.1787e-02,  1.4914e-02,  1.3922e-02,  1.3687e-02,  1.1496e-02,\n",
      "          1.0619e-02,  1.5923e-02,  1.2104e-02,  1.0994e-02,  1.4386e-02,\n",
      "          1.2340e-02,  1.1416e-02,  1.3351e-02,  1.2628e-02,  9.1292e-03,\n",
      "          1.2154e-02,  9.3743e-03,  1.2610e-02,  1.0887e-02,  9.4200e-03],\n",
      "        [ 1.0422e-02,  1.3893e-02,  1.2764e-02,  1.2405e-02,  1.0592e-02,\n",
      "          8.6844e-03,  1.2385e-02,  1.2881e-02,  7.0197e-03,  1.3803e-02,\n",
      "          1.4324e-02,  9.5016e-03,  1.1534e-02,  1.0978e-02,  1.0720e-02,\n",
      "          1.2945e-02,  1.3268e-02,  1.1477e-02,  1.4455e-02,  9.3445e-03],\n",
      "        [ 1.4176e-02,  3.1358e-01,  1.7878e-02,  1.4992e-02,  2.6790e-01,\n",
      "          1.3493e-02,  1.0253e-02,  1.2085e-02,  1.3024e-02, -5.5250e-02,\n",
      "          1.0240e-01,  1.0967e-01, -2.0912e-01, -1.6491e-01,  6.0157e-03,\n",
      "          1.1370e-02,  1.8375e-02, -7.1060e-02, -1.2337e-01,  1.2048e-02],\n",
      "        [ 1.3566e-02,  1.1053e-02,  1.4462e-02,  1.3971e-02,  1.2763e-02,\n",
      "          1.4685e-02,  1.1233e-02,  1.3091e-02,  1.1643e-02,  1.1540e-02,\n",
      "          1.4360e-02,  1.3418e-02,  1.2509e-02,  1.1654e-02,  1.3648e-02,\n",
      "          1.2397e-02,  1.4004e-02,  1.4046e-02,  1.4612e-02,  1.4733e-02],\n",
      "        [ 1.3015e-02,  1.1326e-02,  9.3018e-03,  1.4775e-02,  1.3362e-02,\n",
      "          1.2019e-02,  1.2092e-02,  1.2097e-02,  6.3411e-03,  1.2112e-02,\n",
      "          1.1646e-02,  1.3275e-02,  1.2738e-02,  1.4502e-02,  1.2979e-02,\n",
      "          1.0791e-02,  1.4456e-02,  1.2392e-02,  1.1811e-02,  1.5035e-02],\n",
      "        [-5.0507e-04,  3.1948e-03,  1.8547e-03,  8.9107e-04,  2.8891e-04,\n",
      "         -5.6707e-04,  1.7663e-03,  2.7445e-03,  1.5811e-03, -9.0606e-05,\n",
      "         -1.3216e-03,  1.5814e-03,  2.3620e-03,  1.1760e-03,  9.1380e-04,\n",
      "          1.1998e-03,  3.1697e-05,  2.2893e-03,  2.1510e-03,  2.1852e-03],\n",
      "        [ 9.1399e-04,  7.2340e-04,  1.9937e-03, -2.7796e-04,  4.4962e-04,\n",
      "          2.2981e-03,  2.9792e-03,  4.2448e-03,  6.5092e-04,  9.6509e-04,\n",
      "          8.4093e-04,  1.6287e-03,  2.3516e-03,  6.9944e-04,  2.1830e-03,\n",
      "          1.4311e-03,  1.8038e-03,  2.3532e-03, -6.2169e-04,  8.5243e-04],\n",
      "        [ 2.3458e-03,  1.9665e-03,  4.1687e-05,  3.2512e-03,  2.3671e-03,\n",
      "          1.8077e-03,  2.6648e-03,  2.2011e-03,  1.0976e-03,  2.3880e-03,\n",
      "          2.2446e-03,  1.4161e-03,  1.4561e-03,  2.1679e-03,  1.2774e-03,\n",
      "          1.2696e-03, -1.1698e-03, -1.5503e-04,  2.9367e-03,  6.8386e-03],\n",
      "        [ 1.1957e-03,  1.8823e-03,  1.9543e-03, -8.6624e-04, -1.0162e-03,\n",
      "          3.7531e-03, -8.5214e-04,  1.5351e-03,  2.6856e-03,  1.9409e-03,\n",
      "          1.4146e-03,  2.3259e-03, -3.2045e-03, -7.6090e-05, -1.8291e-04,\n",
      "         -2.0600e-03,  7.6425e-04,  3.0017e-03,  4.9876e-04,  5.4693e-04],\n",
      "        [ 2.5986e-03,  2.2131e-03,  2.0920e-03,  2.6295e-03, -7.5151e-04,\n",
      "          4.1038e-03,  1.4380e-03,  1.9131e-03,  3.3396e-04,  2.7530e-03,\n",
      "          1.3426e-03,  2.6058e-03,  2.9587e-04,  8.8487e-04,  2.2358e-03,\n",
      "          9.0168e-04,  9.9098e-04,  1.2830e-03,  1.5253e-03, -1.5403e-03],\n",
      "        [ 6.7133e-04, -2.5664e-03,  1.9073e-03,  7.4024e-04,  5.4686e-04,\n",
      "          2.0368e-03,  3.1329e-03,  7.7385e-04, -1.9363e-03,  1.9651e-03,\n",
      "          1.0632e-03,  1.1156e-03, -6.2023e-04,  7.7014e-04,  2.2335e-04,\n",
      "         -2.3496e-03,  1.0470e-03, -8.9851e-05, -1.0356e-03,  3.0651e-03],\n",
      "        [-6.2574e-02, -5.9350e-02, -5.7491e-02, -5.8302e-02, -5.7317e-02,\n",
      "         -2.0767e-01, -5.8643e-02, -6.0911e-02, -1.3883e-01, -6.2773e-02,\n",
      "         -5.9971e-02, -6.0196e-02, -7.9446e-02, -5.8051e-02, -6.0968e-02,\n",
      "         -6.2886e-02, -5.7856e-02, -1.1602e-01, -6.5067e-02, -5.7060e-02],\n",
      "        [-5.8840e-02, -6.4941e-02, -5.9676e-02, -6.3461e-02, -6.0043e-02,\n",
      "         -6.3213e-02, -5.7248e-02, -1.0809e-01, -6.1326e-02, -6.1069e-02,\n",
      "         -5.8829e-02, -5.8711e-02, -7.1697e-02, -5.8616e-02, -6.0782e-02,\n",
      "         -7.3249e-02, -5.7568e-02, -5.8484e-02, -6.0905e-02, -6.9313e-01],\n",
      "        [ 1.3881e+00,  1.4940e+00, -5.8248e-02,  1.1847e+00, -1.0146e+00,\n",
      "         -1.6799e+00, -1.6088e+00,  1.6892e+00, -9.2378e-01,  6.6320e-01,\n",
      "          1.6165e+00,  1.5668e+00,  1.5799e+00, -5.7165e-02, -1.7598e-01,\n",
      "         -1.4069e+00,  4.4415e-01, -7.3664e-02,  1.6007e+00, -1.5980e+00],\n",
      "        [ 2.4573e-01, -5.1816e-01, -3.6385e-01, -6.0935e-02, -4.7352e-01,\n",
      "         -5.1156e-01, -4.4928e-01, -6.0026e-02,  3.9355e-03, -7.1147e-02,\n",
      "         -1.1069e-01, -5.2727e-01, -5.8747e-02, -5.9750e-02, -4.3326e-01,\n",
      "         -2.8795e-01, -5.5516e-02, -3.7490e-01, -7.1651e-02,  9.0281e-02],\n",
      "        [-2.1259e-01, -5.7826e-02, -6.1046e-02, -5.7549e-02, -5.7749e-02,\n",
      "         -6.2048e-02, -6.2188e-02, -1.8344e-01, -6.0064e-02, -6.4995e-02,\n",
      "         -2.8134e-01, -5.8889e-02, -6.3095e-02, -5.9865e-02, -6.2895e-02,\n",
      "         -5.9149e-02, -8.6490e-02, -5.9707e-02, -2.8736e-01, -7.1251e-02],\n",
      "        [-5.9635e-02, -6.1935e-02, -5.7812e-02, -5.7840e-02, -5.8705e-02,\n",
      "         -1.4370e-01, -6.3695e-02, -6.0116e-02, -2.6603e-01, -6.6579e-02,\n",
      "         -5.8457e-02, -6.3801e-02, -3.0283e-01, -5.9576e-02, -7.9016e-02,\n",
      "         -6.0834e-02, -6.1488e-02, -6.2146e-02, -6.0452e-02, -3.6292e-01],\n",
      "        [-1.8133e+00,  4.8413e-02,  1.8494e+00, -1.4616e+00,  1.6255e+00,\n",
      "         -1.4877e+00,  1.9800e+00,  1.7897e+00,  2.0324e+00, -1.7673e+00,\n",
      "          1.2411e+00, -1.4609e+00, -1.8119e+00, -1.9166e+00, -1.7309e+00,\n",
      "          2.0224e+00, -2.0170e+00, -1.8467e+00,  1.7829e+00,  2.0112e+00],\n",
      "        [ 5.2927e-02,  5.4548e-02,  5.4455e-02,  5.5809e-02,  5.3915e-02,\n",
      "          5.8345e-02,  5.1815e-02,  5.7976e-02,  5.1249e-02,  5.4042e-02,\n",
      "          5.2335e-02,  5.5205e-02,  5.3613e-02,  5.4004e-02,  5.8422e-02,\n",
      "          5.2904e-02,  5.4758e-02,  5.5746e-02,  5.2000e-02,  3.0307e-01],\n",
      "        [ 5.4985e-02,  5.7381e-02,  5.3906e-02,  5.2699e-02,  5.8241e-02,\n",
      "          5.5695e-02,  5.6106e-02,  5.2354e-02,  5.2234e-02,  5.5700e-02,\n",
      "          5.3157e-02,  5.1534e-02,  5.6671e-02,  5.7854e-02,  5.2521e-02,\n",
      "          5.3565e-02,  5.6011e-02,  1.1399e-01,  4.9837e-02,  5.4539e-02],\n",
      "        [ 5.2550e-02,  6.3651e-02,  5.3004e-02,  4.0931e-02,  5.2390e-02,\n",
      "          8.7108e-02,  5.3609e-02,  5.3935e-02,  2.8949e-01,  5.5432e-02,\n",
      "          5.4171e-02,  4.0060e-01,  3.6545e-01,  5.2302e-02,  2.4940e-01,\n",
      "          5.3990e-02,  3.3949e-02,  6.3281e-02,  3.3967e-01,  1.6940e-01],\n",
      "        [ 5.2851e-02,  5.4755e-02,  5.3152e-02,  5.6733e-02,  5.2142e-02,\n",
      "          5.3888e-02,  5.3247e-02,  5.9691e-02,  5.5903e-02,  5.9021e-02,\n",
      "          5.2774e-02,  5.2389e-02,  5.1560e-02,  5.3415e-02,  5.4519e-02,\n",
      "          5.4079e-02,  5.1568e-02,  5.7001e-02,  5.1455e-02,  5.1596e-02],\n",
      "        [ 5.9383e-02,  5.4170e-02,  5.4879e-02,  5.7464e-02,  5.4337e-02,\n",
      "          5.4539e-02,  5.6935e-02,  5.2282e-02,  5.5180e-02,  5.7149e-02,\n",
      "          5.4547e-02,  5.3964e-02,  6.3931e-02,  5.3282e-02,  5.4799e-02,\n",
      "          5.4500e-02,  5.3336e-02,  5.7589e-02,  5.3396e-02,  5.5447e-02],\n",
      "        [ 2.0672e+00, -2.0174e+00, -5.1235e-01,  2.0779e+00,  2.0771e+00,\n",
      "         -1.7521e+00, -1.8434e+00, -1.4323e+00, -9.4403e-01,  2.0090e+00,\n",
      "          2.0895e+00,  1.9638e+00, -1.8019e+00, -1.8737e+00,  2.1655e+00,\n",
      "          1.9483e+00,  1.9040e-02, -1.9352e+00, -1.8624e+00, -1.8356e+00],\n",
      "        [ 2.7718e-02,  2.3772e-02,  2.5012e-02,  2.6259e-02,  3.2347e-02,\n",
      "          2.8459e-02,  2.5460e-02,  2.6650e-02,  2.5791e-02,  2.6202e-02,\n",
      "          2.7435e-02,  2.5515e-02,  2.4179e-02,  2.5867e-02,  2.6131e-02,\n",
      "          2.5071e-02,  2.5640e-02,  2.7857e-02,  2.2404e-02,  2.4288e-02],\n",
      "        [ 2.5041e-02,  2.6633e-02,  2.5217e-02,  2.4220e-02,  2.7095e-02,\n",
      "          2.7202e-02,  2.5087e-02,  2.7048e-02,  2.5092e-02,  2.6331e-02,\n",
      "          2.5968e-02,  2.4522e-02,  2.4923e-02,  2.6536e-02,  2.4866e-02,\n",
      "          2.3177e-02,  2.7405e-02,  2.4222e-02,  2.3782e-02,  2.7602e-02],\n",
      "        [ 2.5953e-02,  2.3809e-02,  2.4824e-02,  2.7195e-02,  2.3758e-02,\n",
      "          2.4802e-02,  2.6040e-02,  2.6086e-02,  3.0490e-02,  2.4936e-02,\n",
      "          2.5475e-02,  3.0805e-02,  2.6530e-02,  2.5074e-02,  2.5862e-02,\n",
      "          2.7055e-02,  2.6590e-02,  2.2789e-02,  2.5790e-02,  2.8194e-02],\n",
      "        [ 1.9979e+00, -1.6541e+00, -1.9266e+00, -1.8443e+00,  6.5044e-01,\n",
      "          1.0655e-01,  1.9047e+00,  1.9217e+00,  1.8605e+00,  2.2890e+00,\n",
      "         -1.7036e+00,  2.1948e+00,  2.0658e+00, -1.8504e+00,  1.5236e+00,\n",
      "         -2.2537e-01,  1.9819e+00,  2.8285e-02,  2.6797e-02, -8.8904e-01],\n",
      "        [-1.6747e+00,  1.7583e+00, -1.5938e+00,  1.7070e+00,  1.8474e+00,\n",
      "         -1.3258e+00, -1.8669e+00, -1.8976e+00,  1.3559e+00,  2.4551e-02,\n",
      "          2.2505e-02,  1.8631e+00, -1.8875e+00, -1.8041e+00, -1.8718e+00,\n",
      "          2.7865e-02,  1.8453e+00, -1.6378e+00,  1.2263e+00, -1.6948e+00]],\n",
      "       requires_grad=True)\n",
      "tensor([[-1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,\n",
      "          1., -1., -1., -1., -1.,  1.],\n",
      "        [-1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,\n",
      "         -1., -1., -1., -1., -1., -1.],\n",
      "        [-1.,  1., -1., -1., -1., -1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,\n",
      "         -1.,  1.,  1., -1.,  1., -1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,\n",
      "         -1., -1.,  1., -1., -1., -1.],\n",
      "        [ 1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,\n",
      "          1., -1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1.,\n",
      "          1., -1.,  1., -1., -1.,  1.],\n",
      "        [-1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,\n",
      "         -1., -1., -1.,  1.,  1.,  1.],\n",
      "        [-1., -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,\n",
      "          1.,  1.,  1.,  1., -1., -1.],\n",
      "        [ 1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
      "          1.,  1., -1., -1.,  1.,  1.],\n",
      "        [-1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,\n",
      "         -1., -1., -1.,  1., -1., -1.],\n",
      "        [ 1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,\n",
      "          1., -1., -1.,  1.,  1., -1.],\n",
      "        [-1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1.,\n",
      "         -1., -1., -1., -1., -1.,  1.],\n",
      "        [-1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,\n",
      "         -1., -1.,  1., -1., -1.,  1.],\n",
      "        [-1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "         -1., -1.,  1., -1., -1., -1.],\n",
      "        [ 1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         -1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
      "         -1., -1.,  1., -1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "         -1., -1., -1., -1., -1., -1.],\n",
      "        [-1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "         -1., -1., -1., -1., -1., -1.],\n",
      "        [-1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,\n",
      "         -1.,  1., -1., -1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,\n",
      "          1.,  1.,  1.,  1., -1.,  1.],\n",
      "        [ 1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,\n",
      "         -1.,  1.,  1.,  1., -1.,  1.],\n",
      "        [-1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
      "          1.,  1., -1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1.,\n",
      "          1.,  1., -1.,  1., -1., -1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "          1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1., -1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,\n",
      "          1.,  1., -1., -1., -1., -1.],\n",
      "        [ 1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
      "          1., -1.,  1.,  1., -1., -1.],\n",
      "        [-1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
      "         -1., -1.,  1., -1., -1.,  1.],\n",
      "        [ 1., -1., -1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,\n",
      "          1.,  1.,  1., -1.,  1.,  1.],\n",
      "        [ 1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
      "          1., -1.,  1.,  1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
      "         -1.,  1.,  1., -1.,  1., -1.]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "#     print(loss.item())\n",
    "\n",
    "print(losses[-5:])\n",
    "print(model.bitlinear.weight)\n",
    "print(model.bitlinear.binarize_weights_groupwise())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
